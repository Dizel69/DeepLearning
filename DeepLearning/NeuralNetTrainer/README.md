# NeuralNetTrainer

## Описание проекта

Этот проект реализует простую нейронную сеть для классификации изображений из набора данных MNIST с использованием функции активации ReLU. Нейронная сеть обучается различать рукописные цифры от 0 до 9 с использованием собственных весов и алгоритма обратного распространения ошибки.

### Основные особенности:
- Входные данные: изображения размером 28x28 пикселей.
- Один скрытый слой с 40 нейронами.
- Функция активации ReLU и её производная.
- Скорость обучения (alpha): 0.005.
- Количество итераций: 350.


## Требования
- Python 3.x
- NumPy
- Keras (для загрузки набора данных MNIST)

Убедитесь, что установлены все зависимости:
```bash
pip install numpy keras
```

## Структура кода

1. **Импорт библиотек и загрузка данных**
    - Используется модуль `keras.datasets` для загрузки набора данных MNIST.
    - Данные нормализуются и подготавливаются для обучения.

2. **Подготовка данных**
    - Изображения преобразуются в одномерные массивы с диапазоном значений от 0 до 1.
    - Метки переводятся в формат one-hot кодировки для многоцелевой классификации.

3. **Инициализация параметров**
    - `alpha` — скорость обучения.
    - `iterations` — количество итераций обучения.
    - `hidden_size` — количество нейронов в скрытом слое.
    - Веса инициализируются случайным образом в диапазоне [-0.1, 0.1].

4. **Функции активации**
    - `relu` возвращает значение `x`, если `x >= 0`, и 0 в противном случае.
    - `relu2deriv` возвращает 1 для `x >= 0` и 0 иначе (производная ReLU).

5. **Цикл обучения**
    - Для каждой итерации обрабатываются все обучающие примеры.
    - Рассчитываются ошибки, обновляются веса с помощью обратного распространения.
    - Вычисляется общая ошибка и количество правильно классифицированных изображений.

6. **Оценка точности**
    - В конце цикла выводится процент правильно классифицированных изображений.

## Запуск кода
Чтобы запустить модель, выполните следующий скрипт:
```bash
python mnist_relu_neural_net.py
```

Во время выполнения вы увидите обновления об ошибках и точности классификации.

## Возможные улучшения
- Увеличение числа нейронов в скрытом слое.
- Использование других функций активации, таких как сигмоида или tanh.
- Применение регуляризации для уменьшения переобучения.
- Использование оптимизаторов, например, Adam или RMSprop.

## Заключение
Этот проект демонстрирует простую реализацию нейронной сети с прямым распространением и обучением на наборе данных MNIST. Для улучшения производительности можно экспериментировать с параметрами модели и структурами нейронных сетей.

