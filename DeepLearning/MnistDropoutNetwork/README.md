# MNIST Dropout Network

## Описание проекта
Этот проект реализует простую нейронную сеть для классификации изображений из набора данных MNIST с использованием методов активации, регуляризации и оптимизации. Основной акцент сделан на использование **dropout** для борьбы с переобучением. Сеть обучается на подвыборке изображений для демонстрации базовых подходов машинного обучения.

## Основные характеристики
- **Архитектура**: Двухслойная сеть с одним скрытым слоем.
- **Активация**: Используется функция активации Tanh.
- **Регуляризация**: Dropout для уменьшения переобучения.
- **Оптимизация**: Обновление весов с помощью стохастического градиентного спуска (SGD).

## Файлы проекта
- `mnist_dropout_network.py`: Основной скрипт для обучения и тестирования сети.

## Подготовка данных
Используется библиотека `keras.datasets` для загрузки набора данных MNIST.

### Формат данных
- Изображения размером 28x28 пикселей преобразуются в одномерные массивы длиной 784.
- Метки классов представляются в формате one-hot кодировки.

## Основные компоненты кода

### Функции активации
- `tanh(x)`: Возвращает гиперболический тангенс от входного значения.
- `tanh2deriv(output)`: Производная от функции tanh, необходимая для обратного распространения ошибки.
- `softmax(x)`: Нормализует выходные значения для получения вероятностей классов.

### Инициализация параметров
- `alpha`: Скорость обучения.
- `iterations`: Количество итераций обучения.
- `hidden_size`: Размер скрытого слоя.
- `batch_size`: Размер мини-пакета для градиентного спуска.

### Регуляризация с Dropout
Dropout используется для случайного обнуления нейронов скрытого слоя во время обучения:
```python
    dropout_mask = np.random.randint(2, size=layer_1.shape)
    layer_1 *= dropout_mask * 2
```

### Обратное распространение ошибки
Ошибки вычисляются для обновления весов на каждом слое:
- `layer_2_delta`: Ошибка для выходного слоя.
- `layer_1_delta`: Ошибка для скрытого слоя, скорректированная производной tanh и dropout.

### Метрика точности
Выводится точность на тренировочных и тестовых данных каждые 10 итераций.

## Результаты и выводы
- Проект демонстрирует, как простые методы регуляризации могут улучшить качество модели.
- Обучение выполняется за несколько сотен итераций, достигая точности выше 85% на тестовых данных.

## Зависимости
- Python 3
- Numpy
- Keras (для загрузки данных MNIST)

## Запуск программы
```bash
python mnist_dropout_network.py
```

## Заключение
Проект иллюстрирует базовый подход к созданию нейронных сетей с регуляризацией. Он полезен для изучения концепций dropout и градиентного спуска с использованием мини-пакетов.

